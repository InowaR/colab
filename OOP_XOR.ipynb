{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbZh6EJNNNne+/2oPXxOad",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/InowaR/colab/blob/main/OOP_XOR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def mse(y_true, y_pred):\n",
        "    return np.mean(np.power(y_true - y_pred, 2))\n",
        "\n",
        "def mse_prime(y_true, y_pred):\n",
        "    return 2 * (y_pred - y_true) / np.size(y_true)\n",
        "\n",
        "\n",
        "def predict(network, input):\n",
        "    output = input\n",
        "    for layer in network:\n",
        "        output = layer.forward(output)\n",
        "    return output\n",
        "\n",
        "def train(network, loss, loss_prime, x_train, y_train, epochs=1000, learning_rate=0.1, verbose=False):\n",
        "    for e in range(epochs):\n",
        "        error = 0\n",
        "        for x, y in zip(x_train, y_train):\n",
        "            output = predict(network, x)\n",
        "            error += loss(y, output)\n",
        "            grad = loss_prime(y, output)\n",
        "            for layer in reversed(network):\n",
        "                grad = layer.backward(grad, learning_rate)\n",
        "        error /= len(x_train)\n",
        "        if verbose:\n",
        "            print(f\"{e + 1}/{epochs}, error={error}\")\n",
        "    return error\n",
        "\n",
        "\n",
        "class Dense():\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weights = np.random.randn(output_size, input_size)\n",
        "        self.bias = np.random.randn(output_size, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        return np.dot(self.weights, self.input) + self.bias\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        weights_gradient = np.dot(output_gradient, self.input.T)\n",
        "        input_gradient = np.dot(self.weights.T, output_gradient)\n",
        "        self.weights -= learning_rate * weights_gradient\n",
        "        self.bias -= learning_rate * output_gradient\n",
        "        return input_gradient\n",
        "\n",
        "class Activation():\n",
        "    def __init__(self, activation, activation_prime):\n",
        "        self.activation = activation\n",
        "        self.activation_prime = activation_prime\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        return self.activation(self.input)\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        return np.multiply(output_gradient, self.activation_prime(self.input))\n",
        "\n",
        "\n",
        "class Tanh(Activation):\n",
        "    name = 'Tanh'\n",
        "    def __init__(self):\n",
        "        def tanh(x):\n",
        "            return np.tanh(x)\n",
        "\n",
        "        def tanh_prime(x):\n",
        "            return 1 - np.tanh(x) ** 2\n",
        "\n",
        "        super().__init__(tanh, tanh_prime)\n",
        "\n",
        "class Sigmoid(Activation):\n",
        "    name = 'Sigmoid'\n",
        "    def __init__(self):\n",
        "        def sigmoid(x):\n",
        "            return 1 / (1 + np.exp(-x))\n",
        "\n",
        "        def sigmoid_prime(x):\n",
        "            s = sigmoid(x)\n",
        "            return s * (1 - s)\n",
        "\n",
        "        super().__init__(sigmoid, sigmoid_prime)\n",
        "\n",
        "\n",
        "class ReLU(Activation):\n",
        "    name = 'ReLU'\n",
        "    def __init__(self):\n",
        "        def relu(x):\n",
        "            return np.maximum(0, x)\n",
        "\n",
        "        def relu_prime(x):\n",
        "            return (x > 0).astype(np.float32)\n",
        "\n",
        "        super().__init__(relu, relu_prime)\n",
        "\n",
        "\n",
        "\n",
        "x = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "y = [[0], [1], [1], [0]]\n",
        "\n",
        "X = np.reshape(x, (len(x), len(x[0]), 1))\n",
        "Y = np.reshape(y, (len(y), len(y[0]), 1))"
      ],
      "metadata": {
        "id": "FUwFtPVXy-Ws"
      },
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "def get_activation_combinations(objects):\n",
        "    permutations = list(itertools.permutations(objects))\n",
        "    for element in objects:\n",
        "        permutations.append([element, element, element])\n",
        "    return permutations\n",
        "\n",
        "activation_functions = [ReLU(), Tanh(), Sigmoid()]\n",
        "\n",
        "def evaluate_network(network, X, Y):\n",
        "  error = train(network, mse, mse_prime, X, Y)\n",
        "  return error\n",
        "\n",
        "\n",
        "function_configs = get_activation_combinations(activation_functions)\n",
        "\n",
        "best_network = None\n",
        "best_error = np.inf\n",
        "best_first, best_second, best_third = None, None, None\n",
        "\n",
        "\n",
        "for f in function_configs:\n",
        "  network = [\n",
        "      Dense(2, 3),\n",
        "      f[0],\n",
        "      Dense(3, 4),\n",
        "      f[1],\n",
        "      Dense(4, 1),\n",
        "      f[2]\n",
        "  ]\n",
        "\n",
        "  error = evaluate_network(network, X, Y)\n",
        "\n",
        "  if error < best_error:\n",
        "    best_network = network\n",
        "    best_error = error\n",
        "    best_first, best_second, best_third = f[0], f[1], f[2]\n",
        "\n",
        "print(\"Best function configuration:\", best_first.name, best_second.name, best_third.name)\n",
        "print(\"Best training error:\", best_error)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdxXrWYDGCw6",
        "outputId": "3a9612a8-fc11-4eb6-8c6f-4a2cd7556dbf"
      },
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best function configuration: Tanh Sigmoid ReLU\n",
            "Best training error: 2.2340787354891936e-32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_network(network, X, Y):\n",
        "  error = train(network, mse, mse_prime, X, Y)\n",
        "  return error\n",
        "\n",
        "\n",
        "neuron_configs = [(2, 3), (3, 4), (4, 5), (5, 6), (7, 8), (9, 10)]\n",
        "\n",
        "best_network = None\n",
        "best_error = np.inf\n",
        "best_neurons1, best_neurons2 = None, None\n",
        "\n",
        "\n",
        "for neurons1, neurons2 in neuron_configs:\n",
        "  network = [\n",
        "      Dense(2, neurons1),\n",
        "      best_first,\n",
        "      Dense(neurons1, neurons2),\n",
        "      best_second,\n",
        "      Dense(neurons2, 1),\n",
        "      best_third\n",
        "  ]\n",
        "\n",
        "  error = evaluate_network(network, X, Y)\n",
        "\n",
        "  if error < best_error:\n",
        "    best_network = network\n",
        "    best_error = error\n",
        "    best_neurons1, best_neurons2 = neurons1, neurons2\n",
        "\n",
        "\n",
        "print(\"Best neuron configuration:\", best_neurons1, best_neurons2)\n",
        "print(\"Best training error:\", best_error)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfAJqoSs80-E",
        "outputId": "407092b4-d535-4d30-ae82-e36d575407ce"
      },
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best neuron configuration: 4 5\n",
            "Best training error: 5.789826895314128e-33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_network(network, X, Y, lr):\n",
        "  error = train(network, mse, mse_prime, X, Y, learning_rate=lr)\n",
        "  return error\n",
        "\n",
        "\n",
        "learning_rates = [0.1, 0.01, 0.001]\n",
        "\n",
        "best_lr = None\n",
        "best_error = np.inf\n",
        "\n",
        "for lr in learning_rates:\n",
        "    neurons1, neurons2 = best_neurons1, best_neurons2\n",
        "    network = [\n",
        "        Dense(2, neurons1),\n",
        "        best_first,\n",
        "        Dense(neurons1, neurons2),\n",
        "        best_second,\n",
        "        Dense(neurons2, 1),\n",
        "        best_third\n",
        "    ]\n",
        "    error = evaluate_network(network, X, Y, lr)\n",
        "    if error < best_error:\n",
        "      best_lr = lr\n",
        "      best_error = error\n",
        "\n",
        "print(\"Best learning rate:\", best_lr)\n",
        "print(\"Best training error:\", best_error)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQYsrlaOAemk",
        "outputId": "a95b76a1-8a53-4f64-db61-016e1799e080"
      },
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best learning rate: 0.001\n",
            "Best training error: 0.2295512519238576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "neurons1, neurons2 = best_neurons1, best_neurons2\n",
        "\n",
        "network = [\n",
        "    Dense(2, neurons1),\n",
        "    Tanh(),\n",
        "    Dense(neurons1, neurons2),\n",
        "    Tanh(),\n",
        "    Dense(neurons2, 1),\n",
        "    Tanh()\n",
        "]\n",
        "\n",
        "train(network, mse, mse_prime, X, Y, epochs=10000, learning_rate=best_lr, verbose=False)\n",
        "\n",
        "print(predict(network, X[0]))\n",
        "print(predict(network, X[1]))\n",
        "print(predict(network, X[2]))\n",
        "print(predict(network, X[3]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3u-P-IdEYBY",
        "outputId": "42751c17-d60e-4f49-9f27-2e78fc109f50"
      },
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.00290109]]\n",
            "[[0.96515455]]\n",
            "[[0.96572018]]\n",
            "[[0.00206009]]\n"
          ]
        }
      ]
    }
  ]
}