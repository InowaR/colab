{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6cRggncawFNZQI1QYEpPH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/InowaR/colab/blob/main/perceptron_lambda_reg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "x1 = 0.978532\n",
        "x2 = 0.347821\n",
        "\n",
        "y1 = 0.423678\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def test(w1, w2, w3, w4, w5, w6, bias):\n",
        "    s1 = w1 * x1 + w3 * x2 + bias\n",
        "    h1 = sigmoid(s1)\n",
        "    s2 = w2 * x1 + w4 * x2 + bias\n",
        "    h2 = sigmoid(s2)\n",
        "    s3 = w5 * h1 + w6 * h2 + bias\n",
        "    return sigmoid(s3)\n",
        "\n",
        "def func(learning_rate, lambda_reg, epoch):\n",
        "    w1 = np.random.random()\n",
        "    w2 = np.random.random()\n",
        "    w3 = np.random.random()\n",
        "    w4 = np.random.random()\n",
        "    w5 = np.random.random()\n",
        "    w6 = np.random.random()\n",
        "    bias = 1\n",
        "    for _ in range(epoch):\n",
        "        s1 = w1 * x1 + w3 * x2 + bias\n",
        "        h1 = sigmoid(s1)\n",
        "        s2 = w2 * x1 + w4 * x2 + bias\n",
        "        h2 = sigmoid(s2)\n",
        "        s3 = w5 * h1 + w6 * h2 + bias\n",
        "        output = sigmoid(s3)\n",
        "        error = (y1 - output)**2 + lambda_reg * (w1**2 + w2**2 + w3**2 + w4**2 + w5**2 + w6**2)  # L2 регуляризация\n",
        "        dE_dw5 = -2 * (y1 - output) * output * (1 - output) * h1 + 2 * lambda_reg * w5\n",
        "        dE_dw6 = -2 * (y1 - output) * output * (1 - output) * h2 + 2 * lambda_reg * w6\n",
        "        dE_dw1 = -2 * (y1 - output) * output * (1 - output) * w5 * h1 * (1 - h1) * x1 + 2 * lambda_reg * w1\n",
        "        dE_dw2 = -2 * (y1 - output) * output * (1 - output) * w6 * h2 * (1 - h2) * x1 + 2 * lambda_reg * w2\n",
        "        dE_dw3 = -2 * (y1 - output) * output * (1 - output) * w5 * h1 * (1 - h1) * x2 + 2 * lambda_reg * w3\n",
        "        dE_dw4 = -2 * (y1 - output) * output * (1 - output) * w6 * h2 * (1 - h2) * x2 + 2 * lambda_reg * w4\n",
        "        dE_dbias = -2 * (y1 - output) * output * (1 - output)\n",
        "        w5 = w5 - learning_rate * dE_dw5\n",
        "        w6 = w6 - learning_rate * dE_dw6\n",
        "        w1 = w1 - learning_rate * dE_dw1\n",
        "        w2 = w2 - learning_rate * dE_dw2\n",
        "        w3 = w3 - learning_rate * dE_dw3\n",
        "        w4 = w4 - learning_rate * dE_dw4\n",
        "        bias = bias - learning_rate * dE_dbias\n",
        "    return w1, w2, w3, w4, w5, w6, bias\n",
        "\n",
        "best_params = None\n",
        "best_error = np.inf\n",
        "\n",
        "for learning_rate in np.arange(0.1, 1.1, 0.1):\n",
        "    for lambda_reg in np.arange(0.001, 0.011, 0.001):\n",
        "        for epoch in range(100, 1000, 100):\n",
        "            w1, w2, w3, w4, w5, w6, bias = func(learning_rate, lambda_reg, epoch)\n",
        "            a = test(w1, w2, w3, w4, w5, w6, bias)\n",
        "            error = abs(a - y1)\n",
        "            if error < best_error:\n",
        "                best_error = error\n",
        "                best_params = (learning_rate, lambda_reg, epoch)\n",
        "\n",
        "print(\"Best parameters:\", best_params)\n",
        "print(\"Best error:\", best_error)\n",
        "\n",
        "\n",
        "learning_rate = best_params[0]\n",
        "lambda_reg = best_params[1]  # Параметр регуляризации\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def func():\n",
        "    w1 = np.random.random()\n",
        "    w2 = np.random.random()\n",
        "    w3 = np.random.random()\n",
        "    w4 = np.random.random()\n",
        "    w5 = np.random.random()\n",
        "    w6 = np.random.random()\n",
        "    bias = 1\n",
        "    for i in range(best_params[2]):\n",
        "        s1 = w1 * x1 + w3 * x2 + bias\n",
        "        h1 = sigmoid(s1)\n",
        "        s2 = w2 * x1 + w4 * x2 + bias\n",
        "        h2 = sigmoid(s2)\n",
        "        s3 = w5 * h1 + w6 * h2 + bias\n",
        "        output = sigmoid(s3)\n",
        "        if i % 10 == 0:\n",
        "            error = (y1 - output)**2 + lambda_reg * (w1**2 + w2**2 + w3**2 + w4**2 + w5**2 + w6**2)  # L2 регуляризация\n",
        "            print(f'Error: {error:.6f}')\n",
        "        dE_dw5 = -2 * (y1 - output) * output * (1 - output) * h1 + 2 * lambda_reg * w5\n",
        "        dE_dw6 = -2 * (y1 - output) * output * (1 - output) * h2 + 2 * lambda_reg * w6\n",
        "        dE_dw1 = -2 * (y1 - output) * output * (1 - output) * w5 * h1 * (1 - h1) * x1 + 2 * lambda_reg * w1\n",
        "        dE_dw2 = -2 * (y1 - output) * output * (1 - output) * w6 * h2 * (1 - h2) * x1 + 2 * lambda_reg * w2\n",
        "        dE_dw3 = -2 * (y1 - output) * output * (1 - output) * w5 * h1 * (1 - h1) * x2 + 2 * lambda_reg * w3\n",
        "        dE_dw4 = -2 * (y1 - output) * output * (1 - output) * w6 * h2 * (1 - h2) * x2 + 2 * lambda_reg * w4\n",
        "        dE_dbias = -2 * (y1 - output) * output * (1 - output)\n",
        "        w5 = w5 - learning_rate * dE_dw5\n",
        "        w6 = w6 - learning_rate * dE_dw6\n",
        "        w1 = w1 - learning_rate * dE_dw1\n",
        "        w2 = w2 - learning_rate * dE_dw2\n",
        "        w3 = w3 - learning_rate * dE_dw3\n",
        "        w4 = w4 - learning_rate * dE_dw4\n",
        "        bias = bias - learning_rate * dE_dbias\n",
        "    return w1, w2, w3, w4, w5, w6, bias\n",
        "\n",
        "w1, w2, w3, w4, w5, w6, bias = func()\n",
        "\n",
        "def test():\n",
        "    s1 = w1 * x1 + w3 * x2 + bias\n",
        "    h1 = sigmoid(s1)\n",
        "    s2 = w2 * x1 + w4 * x2 + bias\n",
        "    h2 = sigmoid(s2)\n",
        "    s3 = w5 * h1 + w6 * h2 + bias\n",
        "    output = sigmoid(s3)\n",
        "    return output\n",
        "\n",
        "print(f'Prediction {test()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_09o1igDCzEm",
        "outputId": "1741ed87-29bf-4b97-e49e-84d98f8fd92d"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: (1.0, 0.010000000000000002, 900)\n",
            "Best error: 2.7725611306816944e-08\n",
            "Error: 0.259374\n",
            "Error: 0.018506\n",
            "Error: 0.010245\n",
            "Error: 0.006960\n",
            "Error: 0.004747\n",
            "Error: 0.003243\n",
            "Error: 0.002221\n",
            "Error: 0.001523\n",
            "Error: 0.001047\n",
            "Error: 0.000721\n",
            "Error: 0.000498\n",
            "Error: 0.000344\n",
            "Error: 0.000239\n",
            "Error: 0.000166\n",
            "Error: 0.000116\n",
            "Error: 0.000081\n",
            "Error: 0.000057\n",
            "Error: 0.000040\n",
            "Error: 0.000028\n",
            "Error: 0.000020\n",
            "Error: 0.000014\n",
            "Error: 0.000010\n",
            "Error: 0.000007\n",
            "Error: 0.000005\n",
            "Error: 0.000004\n",
            "Error: 0.000003\n",
            "Error: 0.000002\n",
            "Error: 0.000001\n",
            "Error: 0.000001\n",
            "Error: 0.000001\n",
            "Error: 0.000001\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Error: 0.000000\n",
            "Prediction 0.42367802339793365\n"
          ]
        }
      ]
    }
  ]
}